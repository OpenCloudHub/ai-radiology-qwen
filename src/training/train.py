"""
Training Entry Point
====================

Main entry point for Qwen2.5-VL fine-tuning using Ray Train.

Orchestrates: config loading ‚Üí DVC data fetch ‚Üí model setup ‚Üí
training ‚Üí MLflow model registration.

Functions
---------
train_driver : Orchestrates training from head node (MLflow, Ray setup)
train_worker : Executes training on GPU workers (forward/backward passes)
load_model : Loads Qwen2.5-VL with optional quantization
apply_lora : Applies LoRA/QLoRA adapters to model

Usage
-----
    $ python train.py --config configs/debug_qlora.yaml
    $ ray job submit --working-dir . -- python src/training/train.py --config configs/qlora.yaml

See Also
--------
README.md : Full architecture documentation and environment setup
configs/ : Training configuration examples (qlora, lora, full)
"""

import argparse
import os
from importlib.metadata import version
from pathlib import Path

import mlflow
import ray
import ray.train.huggingface.transformers
import torch
from peft import LoraConfig, TaskType, get_peft_model
from ray.train import (
    CheckpointConfig,
    FailureConfig,
    RunConfig,
    ScalingConfig,
    get_checkpoint,
    get_context,
)
from ray.train.torch import TorchTrainer
from transformers import (
    AutoProcessor,
    AutoTokenizer,
    BitsAndBytesConfig,
    Qwen2_5_VLForConditionalGeneration,
)

from src.training.callbacks import RayTrainReportCallback, load_metadata_from_dvc
from src.training.config import InfraConfig, TrainConfig
from src.training.data import create_dataset_and_collator
from src.training.dvc_loader import load_data_from_dvc
from src.training.log import (
    get_logger,
    log_error,
    log_info,
    log_key_value,
    log_results_summary,
    log_section,
    log_success,
    log_training_summary,
    log_warning,
)
from src.training.mlflow_wrapper import QwenVLPyFuncModel
from src.training.trainer import QwenTrainer


# =============================================================================
# Utility Functions
# =============================================================================
def check_mlflow_env():
    """Check MLflow S3 environment variables are set."""
    required = ["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY", "MLFLOW_S3_ENDPOINT_URL"]
    missing = [var for var in required if var not in os.environ]
    if missing:
        raise RuntimeError(f"Missing required env vars: {missing}")


# =============================================================================
# Model Loading
# =============================================================================
def load_model(config: TrainConfig):
    """Load Qwen2.5-VL model with optional quantization."""
    logger = get_logger(__name__)

    model_name = config.model.name
    training_cfg = config.training

    logger.info("Loading model", extra={"model": model_name})

    # Setup quantization if enabled
    quantization_config = None
    if training_cfg.quantization.enabled:
        logger.info(
            "Quantization enabled",
            extra={
                "type": training_cfg.quantization.type.value,
                "compute_dtype": training_cfg.quantization.compute_dtype,
            },
        )
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=training_cfg.quantization.load_in_4bit,
            load_in_8bit=training_cfg.quantization.load_in_8bit,
            bnb_4bit_use_double_quant=training_cfg.quantization.double_quant,
            bnb_4bit_quant_type=training_cfg.quantization.type.value,
            bnb_4bit_compute_dtype=getattr(
                torch, training_cfg.quantization.compute_dtype
            ),
        )

    # Attention implementation
    attn_impl = (
        "flash_attention_2" if training_cfg.optimization.flash_attention else "eager"
    )

    # Load model
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_name,
        attn_implementation=attn_impl,
        dtype=config.get_torch_dtype(),
        quantization_config=quantization_config,
        trust_remote_code=True,
    )
    model.config.use_cache = False

    # Enable gradient checkpointing
    if training_cfg.optimization.gradient_checkpointing:
        if hasattr(model, "enable_input_require_grads"):
            model.enable_input_require_grads()
        else:

            def make_inputs_require_grad(module, input, output):
                output.requires_grad_(True)

            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        model_max_length=training_cfg.max_length,
        padding_side="right",
        use_fast=False,
        trust_remote_code=True,
    )

    # Load processor
    processor = AutoProcessor.from_pretrained(
        model_name,
        trust_remote_code=True,
    )

    return model, tokenizer, processor


def apply_lora(model, config: TrainConfig):
    """Apply LoRA adapters to model if enabled."""
    logger = get_logger(__name__)
    lora_cfg = config.training.lora

    if not lora_cfg.enabled:
        return model

    logger.info("Applying LoRA", extra={"r": lora_cfg.r, "alpha": lora_cfg.alpha})

    # Determine target modules
    if lora_cfg.target_modules.value == "custom":
        target_modules = lora_cfg.custom_target_modules
    elif lora_cfg.target_modules.value == "all-linear":
        target_modules = "all-linear"
    else:
        target_modules = ["q_proj", "k_proj", "v_proj"]

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=lora_cfg.r,
        lora_alpha=lora_cfg.alpha,
        lora_dropout=lora_cfg.dropout,
        target_modules=target_modules,
    )

    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    return model


# =============================================================================
# Training Functions
# =============================================================================
def train_worker(train_loop_config: dict):
    """Training function that runs on each Ray worker."""
    logger = get_logger(__name__)

    # Unpack configs
    config = TrainConfig.model_validate(train_loop_config["config_dict"])
    infra = InfraConfig()  # Always from env vars
    mlflow_run_id = train_loop_config["mlflow_run_id"]

    rank = get_context().get_world_rank()
    is_rank0 = rank == 0

    if is_rank0:
        log_section("Worker Initialized", "üîß")
        check_mlflow_env()
        mlflow.set_tracking_uri(infra.mlflow_tracking_uri)
        mlflow.set_experiment(infra.mlflow_experiment_name)
        mlflow.start_run(run_id=mlflow_run_id)
        logger.info("Connected to MLflow run", extra={"run_id": mlflow_run_id})

    # Verify GPU
    if torch.cuda.is_available():
        if is_rank0:
            log_info(f"GPU: {torch.cuda.get_device_name()}")
    else:
        raise RuntimeError("CUDA not available")

    # Download data from DVC
    if is_rank0:
        log_section("Loading Data from DVC", "üì¶")

    local_path, metadata = load_data_from_dvc(
        repo=infra.dvc_repo_url,
        version=infra.dvc_data_version,
        processed_path=infra.dvc_processed_path,
        metadata_path=infra.dvc_metadata_path,
    )

    # Update config with downloaded path
    config.data.dataset_path = str(local_path)

    # Prompt info is required
    prompt_info = metadata.get("prompt")
    if prompt_info is None:
        raise RuntimeError(
            f"No prompt info in metadata for DVC version {infra.dvc_data_version}"
        )

    if is_rank0:
        log_key_value("DVC Version", infra.dvc_data_version)
        log_key_value(
            "Prompt", f"{prompt_info['prompt_name']} v{prompt_info['prompt_version']}"
        )

    # Load model
    if is_rank0:
        log_section("Loading Model", "ü§ñ")
    model, tokenizer, processor = load_model(config)

    # Apply LoRA if enabled
    model = apply_lora(model, config)

    # Create datasets
    if is_rank0:
        log_section("Preparing Data", "üì¶")
    data_module = create_dataset_and_collator(
        tokenizer=tokenizer,
        processor=processor,
        data_config=config.data,
    )

    # Create HuggingFace TrainingArguments
    hf_training_args = config.training.to_hf_training_args()

    # Create trainer
    trainer = QwenTrainer(
        config=config,
        model=model,
        args=hf_training_args,
        processing_class=tokenizer,
        train_dataset=data_module["train_dataset"],
        eval_dataset=data_module["eval_dataset"],
        data_collator=data_module["data_collator"],
    )

    if is_rank0:
        trainer.print_model_status()

    # Add Ray callback with required processor and prompt_info
    callback = RayTrainReportCallback(processor=processor, prompt_info=prompt_info)
    trainer.add_callback(callback)

    # Prepare for training
    trainer = ray.train.huggingface.transformers.prepare_trainer(trainer)

    # Check for existing checkpoint
    if is_rank0:
        log_section("Training", "üèÉ")

    checkpoint = get_checkpoint()
    if checkpoint:
        logger.info("Found Ray checkpoint", extra={"checkpoint": str(checkpoint)})
        with checkpoint.as_directory() as ckpt_dir:
            ckpt_path = Path(ckpt_dir)
            has_lora = (ckpt_path / "adapter_config.json").exists()
            has_full = (ckpt_path / "config.json").exists()
            has_trainer = (ckpt_path / "trainer_state.json").exists()

            if has_trainer and (has_lora or has_full):
                log_info(f"Resuming from: {ckpt_path}")
                trainer.train(resume_from_checkpoint=str(ckpt_path))
            else:
                log_warning("Invalid checkpoint, starting fresh")
                trainer.train()
    else:
        log_info("Starting training from scratch")
        trainer.train()

    if is_rank0:
        log_success("Training completed on rank 0")


def train_driver(config: TrainConfig, infra: InfraConfig) -> ray.train.Result:
    """Driver function that orchestrates training."""
    log_section("Qwen2.5-VL Training Pipeline", "üöÄ")
    logger = get_logger(__name__)

    # Load metadata to get prompt info for tagging
    log_info("Loading metadata from DVC...")
    metadata = load_metadata_from_dvc(
        repo=infra.dvc_repo_url,
        version=infra.dvc_data_version,
        metadata_path=infra.dvc_metadata_path,
    )
    prompt_info = metadata.get("prompt")
    if prompt_info is None:
        raise RuntimeError(f"No prompt info in metadata for {infra.dvc_data_version}")

    # Show training configuration
    log_training_summary(
        model=config.model.name,
        method=config.training_method_summary,
        dataset=f"DVC: {infra.dvc_data_version}",
        steps=config.training.max_steps,
        batch_size=config.training.batch_size,
        lr=config.training.learning_rate,
    )

    # Setup MLflow
    log_section("MLflow Setup", "üìä")
    check_mlflow_env()
    mlflow.set_tracking_uri(infra.mlflow_tracking_uri)
    mlflow.set_experiment(infra.mlflow_experiment_name)

    log_key_value("Tracking URI", infra.mlflow_tracking_uri)
    log_key_value("Experiment", infra.mlflow_experiment_name)

    # Workflow tags from infra
    workflow_tags = {
        "argo_workflow_uid": infra.argo_workflow_uid,
        "docker_image_tag": infra.docker_image_tag,
        "dvc_data_version": infra.dvc_data_version,
        "prompt_name": prompt_info["prompt_name"],
        "prompt_version": str(prompt_info["prompt_version"]),
    }

    with mlflow.start_run(tags=workflow_tags) as active_run:
        mlflow_run_id = active_run.info.run_id
        log_success(f"MLflow run started: {mlflow_run_id}")

        # Log configuration
        mlflow.log_params(
            {
                "model_name": config.model.name,
                "training_method": config.training_method_summary,
                "batch_size": config.training.batch_size,
                "learning_rate": config.training.learning_rate,
                "max_steps": config.training.max_steps,
                "lora_enabled": config.training.lora.enabled,
                "lora_r": config.training.lora.r
                if config.training.lora.enabled
                else None,
                "quantization_enabled": config.training.quantization.enabled,
                "flash_attention": config.training.optimization.flash_attention,
                "dvc_data_version": infra.dvc_data_version,
            }
        )

        # Prepare train loop config
        train_loop_config = {
            "config_dict": config.model_dump(),
            "mlflow_run_id": mlflow_run_id,
        }

        # Create Ray TorchTrainer
        log_section("Ray Trainer Setup", "‚ö°")
        log_key_value("Workers", infra.ray_num_workers)
        log_key_value("Storage", infra.ray_storage_path)

        ray_trainer = TorchTrainer(
            train_loop_per_worker=train_worker,
            train_loop_config=train_loop_config,
            scaling_config=ScalingConfig(
                num_workers=infra.ray_num_workers,
                use_gpu=infra.ray_gpu_fraction > 0,
                resources_per_worker={"GPU": infra.ray_gpu_fraction}
                if infra.ray_gpu_fraction > 0
                else {},
            ),
            run_config=RunConfig(
                name=f"run_{mlflow_run_id}",
                storage_path=infra.ray_storage_path,
                checkpoint_config=CheckpointConfig(
                    num_to_keep=1,
                    checkpoint_score_attribute="loss",
                    checkpoint_score_order="min",
                ),
                failure_config=FailureConfig(max_failures=3),
            ),
        )

        # Run training
        log_section("Run Training", "üèãÔ∏è")
        result = ray_trainer.fit()

        # Clear GPU memory
        torch.cuda.empty_cache()

        # Log model to MLflow from best checkpoint
        log_section("Model Registration", "üì¶")

        if result.checkpoint:
            log_info(f"Best checkpoint: {result.checkpoint}")

            with result.checkpoint.as_directory() as checkpoint_dir:
                checkpoint_path = Path(checkpoint_dir)

                # Validate checkpoint
                has_full_model = (checkpoint_path / "config.json").exists()
                has_lora = (checkpoint_path / "adapter_config.json").exists()
                has_processor = (checkpoint_path / "preprocessor_config.json").exists()
                has_prompt = (checkpoint_path / "prompt_info.json").exists()

                if not has_processor:
                    raise RuntimeError("Missing preprocessor_config.json in checkpoint")
                if not has_prompt:
                    raise RuntimeError("Missing prompt_info.json in checkpoint")
                if not has_full_model and not has_lora:
                    raise RuntimeError("Missing model files in checkpoint")

                checkpoint_type = "LoRA" if has_lora else "Full"
                log_info(f"Found {checkpoint_type} checkpoint, logging to MLflow...")

                # Force CPU loading for MLflow validation
                os.environ["MLFLOW_MODEL_DEVICE"] = "cpu"

                try:
                    # Log model (signature inferred from type hints)
                    model_info = mlflow.pyfunc.log_model(
                        name="model",
                        python_model=QwenVLPyFuncModel(),
                        artifacts={"model": str(checkpoint_path)},
                        pip_requirements=[
                            f"transformers=={version('transformers')}",
                            f"torch=={version('torch')}",
                            f"peft=={version('peft')}",
                            f"pydantic=={version('pydantic')}",
                            "pillow",
                            "accelerate",
                            "bitsandbytes",
                        ],
                        registered_model_name=infra.mlflow_registered_model_name,
                    )

                    log_success(
                        f"Model registered: {infra.mlflow_registered_model_name}"
                    )
                    log_key_value("Model URI", model_info.model_uri)

                finally:
                    os.environ.pop("MLFLOW_MODEL_DEVICE", None)
        else:
            raise RuntimeError(
                "No checkpoint available - training failed to produce checkpoint"
            )

    return result


# =============================================================================
# Main Entry Point
# =============================================================================
def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Train Qwen2.5-VL model")
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="Path to YAML config file",
    )
    args = parser.parse_args()

    log_section("Configuration", "‚öôÔ∏è")

    # Infrastructure from env vars (CI controls)
    log_info("Loading infrastructure config from environment")
    infra = InfraConfig()
    log_key_value("DVC Version", infra.dvc_data_version)
    log_key_value("MLflow URI", infra.mlflow_tracking_uri)
    log_key_value("Experiment", infra.mlflow_experiment_name)

    # Training params from YAML (devs control)
    log_info(f"Loading training config from: {args.config}")
    config = TrainConfig.from_yaml(args.config)

    # Initialize Ray
    if not ray.is_initialized():
        ray.init()
        log_success("Ray initialized")

    # Run training
    result = train_driver(config, infra)

    # Print results
    log_section("Results", "üèÅ")

    if result.error:
        log_error(f"Training failed: {result.error}")
        raise RuntimeError(f"Training failed: {result.error}")

    log_results_summary(
        metrics=result.metrics or {},
        checkpoint_path=str(result.checkpoint) if result.checkpoint else None,
    )
    log_success("Training pipeline complete!")


if __name__ == "__main__":
    main()
