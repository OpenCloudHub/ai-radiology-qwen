# LoRA without quantization - compare loss vs QLoRA (10 steps)
# VRAM: ~12-14 GB
data:
  do_train: true
  do_eval: false

model:
  name: "Qwen/Qwen2.5-VL-3B-Instruct"
  tune_vision: false
  tune_mlp: true
  tune_llm: false

training:
  output_dir: "./outputs/debug_lora"
  max_steps: 10
  batch_size: 1
  gradient_accumulation_steps: 1
  learning_rate: 2.0e-4
  warmup_steps: 2
  logging_steps: 2
  save_steps: 10

  lora:
    enabled: true
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules: "all-linear"

  quantization:
    enabled: false

  optimization:
    flash_attention: false
    gradient_checkpointing: true
    bf16: true
