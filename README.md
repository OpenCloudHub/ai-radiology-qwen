<a id="readme-top"></a>

<!-- PROJECT LOGO & TITLE -->

<div align="center">
  <a href="https://github.com/opencloudhub">
  <picture>
    <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/opencloudhub/.github/main/assets/brand/assets/logos/primary-logo-light.svg">
    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/opencloudhub/.github/main/assets/brand/assets/logos/primary-logo-dark.svg">
    <!-- Fallback -->
    <img alt="OpenCloudHub Logo" src="https://raw.githubusercontent.com/opencloudhub/.github/main/assets/brand/assets/logos/primary-logo-dark.svg" style="max-width:700px; max-height:175px;">
  </picture>
  </a>

<h1 align="center">Qwen2.5-VL Radiology - MLOps Demo</h1>

<p align="center">
    Vision-Language Model fine-tuning for radiology image captioning, demonstrating multimodal MLOps with Ray, MLflow, and DVC.<br />
    <a href="https://github.com/opencloudhub"><strong>Explore OpenCloudHub Â»</strong></a>
  </p>
</div>

______________________________________________________________________

<details>
  <summary>ğŸ“‘ Table of Contents</summary>
  <ol>
    <li><a href="#about">About</a></li>
    <li><a href="#thesis-context">Thesis Context</a></li>
    <li><a href="#features">Features</a></li>
    <li><a href="#architecture">Architecture</a></li>
    <li><a href="#getting-started">Getting Started</a></li>
    <li><a href="#configuration">Configuration</a></li>
    <li><a href="#data-pipeline">Data Pipeline</a></li>
    <li><a href="#training">Training</a></li>
    <li><a href="#serving">Serving</a></li>
    <li><a href="#project-structure">Project Structure</a></li>
    <li><a href="#contributing">Contributing</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
  </ol>
</details>

______________________________________________________________________

<h2 id="about">ğŸ¥ About</h2>

This repository demonstrates fine-tuning [Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct) for radiology image captioning using the [ROCO dataset](https://huggingface.co/datasets/unsloth/Radiology_mini). It serves as part of the OpenCloudHub project and an accompanying master's thesis on MLOps, showcasing how to integrate Vision-Language Models into a reproducible ML pipeline.

**Why this exists:**

- MLflow doesn't natively support Vision-Language Models - this shows how to make it work using a custom PyFunc wrapper
- Demonstrates multimodal AI (image + text) in an MLOps context
- Shows how to combine HuggingFace Transformers with Ray for GPU training
- Provides a reference for QLoRA/LoRA fine-tuning with proper experiment tracking
- Illustrates data versioning with DVC and prompt tracking through the full pipeline

This is not a production system - it's a learning resource and integration showcase.

______________________________________________________________________

<h2 id="thesis-context">ğŸ“š Thesis Context</h2>

<!-- TODO:  -->

### Key Technical Contributions

| Challenge                 | Solution                            | Location                         |
| ------------------------- | ----------------------------------- | -------------------------------- |
| VLM experiment tracking   | Custom MLflow PyFunc wrapper        | `src/training/mlflow_wrapper.py` |
| Prompt-model consistency  | Prompt baked into checkpoint        | `src/training/callbacks.py`      |
| Memory-efficient training | QLoRA with gradient checkpointing   | `src/training/trainer.py`        |
| Memory-efficient serving  | 4-bit quantized inference           | `SERVE_QUANTIZED=true` env var   |
| Reproducible data         | DVC versioning with metadata        | `src/training/dvc_loader.py`     |
| Config separation         | Env vars (infra) vs YAML (training) | `src/training/config.py`         |

### Reading the Code

For thesis reviewers, the recommended reading order is:

1. **[config.py](src/training/config.py)** - Understand the configuration philosophy (infra vs training separation)
1. **[train.py](src/training/train.py)** - Entry point showing the driver-worker pattern
1. **[mlflow_wrapper.py](src/training/mlflow_wrapper.py)** - The key innovation: PyFunc wrapper for VLMs
1. **[callbacks.py](src/training/callbacks.py)** - How prompts and processors are bundled with checkpoints
1. **[serve.py](src/serving/serve.py)** - How models are loaded and served via Ray Serve

______________________________________________________________________

<h2 id="features">âœ¨ Features</h2>

- ğŸ–¼ï¸ **Multimodal Training**: Fine-tune Qwen2.5-VL for image-to-text tasks
- âš¡ **QLoRA/LoRA Support**: Memory-efficient fine-tuning with 4-bit quantization
- ğŸ“Š **MLflow Integration**: Custom PyFunc wrapper for VLM tracking and serving
- ğŸ“¦ **DVC Data Pipeline**: Versioned datasets with prompt tracking through lineage
- ğŸ”€ **Ray Train**: Distributed GPU training with fault tolerance and checkpointing
- ğŸš€ **Ray Serve**: Scalable inference with hot model reloading
- âš™ï¸ **Config Separation**: Infrastructure (env vars) vs training params (YAML)
- ğŸ³ **Containerized**: Docker-based training for reproducibility

______________________________________________________________________

<h2 id="architecture">ğŸ—ï¸ Architecture</h2>

### System Overview

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Data Pipeline (DVC)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HuggingFace  â”€â”€â–º  Download  â”€â”€â–º  Process  â”€â”€â–º  Analyze             â”‚
â”‚  Dataset           (raw)         (+ prompt)     (metadata)          â”‚
â”‚                                      â”‚                              â”‚
â”‚             MLflow Prompt Registry / DVC Data Registry              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Training (Ray Train)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Load DVC Data  â”€â”€â–º  Qwen2.5-VL  â”€â”€â–º  QLoRA/LoRA  â”€â”€â–º  Checkpoint   â”‚
â”‚  (with prompt)       + Processor      Fine-tuning      + Processor  â”‚
â”‚                                                        + Prompt     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Model Registry (MLflow)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PyFunc Wrapper  â”€â”€â–º  Artifacts (checkpoint)  â”€â”€â–º  Model Versions   â”‚
â”‚  (custom VLM)         + prompt_info.json           dev.model/1      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Serving (Ray Serve)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Load from MLflow  â”€â”€â–º  QwenVLDeployment  â”€â”€â–º  /predict (base64)    â”‚
â”‚  (model + prompt)       FastAPI + scaling       JSON response       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Architecture (Ray Train)

The training uses a **driver-worker pattern** with single-GPU execution:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        HEAD NODE (Driver)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  train_driver()                                            â”‚  â”‚
â”‚  â”‚  â€¢ Downloads data from DVC (once)                          â”‚  â”‚
â”‚  â”‚  â€¢ Creates MLflow run with tags                            â”‚  â”‚
â”‚  â”‚  â€¢ Configures Ray TorchTrainer                             â”‚  â”‚
â”‚  â”‚  â€¢ Registers final model to MLflow                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GPU WORKER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  train_worker()                                            â”‚  â”‚
â”‚  â”‚  â€¢ Loads model with QLoRA/LoRA                             â”‚  â”‚
â”‚  â”‚  â€¢ Creates PyTorch Dataset from local path                 â”‚  â”‚
â”‚  â”‚  â€¢ Runs HuggingFace Trainer                                â”‚  â”‚
â”‚  â”‚  â€¢ Reports checkpoints to Ray                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Note:** This demo uses single-GPU training. For distributed DDP training, one would load
> only shards of data in the worker nodes. An example of this using
> Ray Data for distributed training with data sharding,
> see [Ray Data with PyTorch Lightning](https://github.com/OpenCloudHub/ai-dl-lightning).

### Serving Architecture (Ray Serve)

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â–¶â”‚    Ray Serve     â”‚â”€â”€â”€â”€â–¶â”‚     MLflow      â”‚
â”‚  (FastAPI)  â”‚     â”‚   (Scaling)      â”‚     â”‚    (Models)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚
       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
       â”‚              â–¼             â–¼
       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚         â”‚Replica â”‚   â”‚Replica â”‚  (Autoscaling)
       â”‚         â”‚  0.25  â”‚   â”‚  0.25  â”‚  (Fractional GPU)
       â”‚         â”‚  GPU   â”‚   â”‚  GPU   â”‚
       â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
  Endpoints:
  GET  /        â†’ Service info
  GET  /health  â†’ Kubernetes readiness probe
  GET  /info    â†’ Model metadata from MLflow
  POST /predict â†’ Image analysis (file upload)
```

### Key Integration Points

1. **Prompt Lineage**: Prompts are versioned in MLflow, embedded during data processing, tracked through training, and used at inference
1. **Checkpoint Contents**: Model weights + processor + prompt_info.json (everything needed to serve)
1. **MLflow PyFunc**: Custom wrapper handles VLM loading since MLflow doesn't support vision-language natively
1. **Quantized Serving**: `SERVE_QUANTIZED=true` enables 4-bit inference (~4GB vs ~8GB VRAM)

______________________________________________________________________

<h2 id="getting-started">ğŸš€ Getting Started</h2>

### Prerequisites

- Docker with NVIDIA Container Toolkit (for GPU training)
- VS Code with DevContainers extension (recommended)
- Access to MLflow tracking server and MinIO (S3-compatible storage)

### Setup

1. **Clone the repository**

```bash
git clone https://github.com/opencloudhub/ai-radiology-qwen.git
cd ai-radiology-qwen
```

2. **Open in DevContainer** (Recommended)

   VSCode: `Ctrl+Shift+P` â†’ `Dev Containers: Rebuild and Reopen in Container`

   Or **setup locally**:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync --dev
```

3. **Configure environment**

Two environment files are provided:

| File            | Use Case                                           |
| --------------- | -------------------------------------------------- |
| `.env.docker`   | Local Docker Compose (MLflow + MinIO on localhost) |
| `.env.minikube` | Minikube/Kubernetes (internal cluster URLs)        |

```bash
# For local Docker Compose setup
set -a && source .env.docker && set +a

# Or for Minikube
set -a && source .env.minikube && set +a
```

See [Configuration](#configuration) for details on all environment variables.

4. **Start Ray**

```bash
ray start --head --num-gpus 1 --num-cpus 12
```

______________________________________________________________________

<h2 id="configuration">âš™ï¸ Configuration</h2>

Configuration is split into two categories by design - this separation is intentional:

### Infrastructure Config (Environment Variables)

**CI/CD controls these** - developers should not override via YAML. This ensures reproducibility and prevents accidental use of wrong endpoints.

| Variable                       | Description                   | Required         |
| ------------------------------ | ----------------------------- | ---------------- |
| `DVC_DATA_VERSION`             | Data version tag              | **Yes**          |
| `DVC_REPO`                     | DVC repository URL            | No (has default) |
| `MLFLOW_TRACKING_URI`          | MLflow server URL             | No (has default) |
| `MLFLOW_EXPERIMENT_NAME`       | Experiment name               | No (has default) |
| `MLFLOW_REGISTERED_MODEL_NAME` | Model registry name           | No (has default) |
| `RAY_NUM_WORKERS`              | Number of Ray workers         | No (default: 1)  |
| `RAY_GPU_FRACTION`             | GPU fraction per worker (0-1) | No (default: 1)  |
| `ARGO_WORKFLOW_UID`            | Workflow tracking ID          | No               |
| `DOCKER_IMAGE_TAG`             | Image tag for reproducibility | No               |

### Training Config (YAML)

**Developers control these** via config files in `configs/`:

```yaml
# configs/qlora.yaml
data:
  max_pixels: 451584      # Max image resolution
  min_pixels: 12544       # Min image resolution
  sampling_percent: 1.0   # Use 100% of data

model:
  name: "Qwen/Qwen2.5-VL-3B-Instruct"
  tune_vision: false      # Freeze vision encoder
  tune_mlp: true          # Train vision-language connector
  tune_llm: false         # Freeze language model

training:
  max_steps: 100
  batch_size: 1
  learning_rate: 2.0e-4
  mm_projector_lr: 2.0e-5  # Separate LR for projector

  lora:
    enabled: true
    r: 64                  # LoRA rank
    alpha: 128             # LoRA alpha
    target_modules: "all-linear"

  quantization:
    enabled: true          # Enable 4-bit quantization
    type: "nf4"            # NormalFloat4
    double_quant: true     # Double quantization

  optimization:
    gradient_checkpointing: true  # Save memory
    flash_attention: false        # Requires flash-attn package
    bf16: true
```

______________________________________________________________________

<h2 id="data-pipeline">ğŸ“¦ Data Pipeline</h2>

Data preparation and versioning is managed in a separate repository using DVC pipelines.

### External Data Registry

The data pipeline lives at: [**OpenCloudHub/data-registry**](https://github.com/OpenCloudHub/data-registry/tree/main/pipelines/roco-radiology)

This separation allows:

- **Decoupled versioning**: Data versions are independent of model code
- **Reusable pipelines**: Multiple training projects can share the same data
- **Clear lineage**: DVC tracks data provenance with prompt metadata

### Pipeline Stages

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Download  â”‚â”€â”€â”€â”€â–¶â”‚  Process   â”‚â”€â”€â”€â”€â–¶â”‚  Analyze   â”‚
â”‚            â”‚     â”‚            â”‚     â”‚            â”‚
â”‚ HuggingFaceâ”‚     â”‚ + Prompt   â”‚     â”‚ Statistics â”‚
â”‚ â†’ Images   â”‚     â”‚ â†’ Qwen fmt â”‚     â”‚ â†’ metadata â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–²
                         â”‚
                   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                   â”‚  MLflow   â”‚
                   â”‚  Prompt   â”‚
                   â”‚  Registry â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Prompt Tracking

Prompts are versioned in MLflow and flow through the entire pipeline:

```text
MLflow Registry â†’ DVC Process â†’ metadata.json â†’ Training â†’ Checkpoint â†’ Serving
```

The model always uses the exact prompt it was trained with - this prevents train-serve skew.

______________________________________________________________________

<h2 id="training">ğŸ‹ï¸ Training</h2>

### Quick Start

```bash
# Local training
python src/training/train.py --config configs/debug_qlora.yaml

# Via Ray Job API (closer to production)
RAY_ADDRESS='http://127.0.0.1:8265' ray job submit \
  --working-dir . \
  -- python src/training/train.py --config configs/debug_qlora_flash.yaml
```

### Training Methods

| Method    | Config                                            | VRAM   | Use Case                |
| --------- | ------------------------------------------------- | ------ | ----------------------- |
| **QLoRA** | `lora.enabled=true`, `quantization.enabled=true`  | ~9GB   | Default, single GPU     |
| LoRA      | `lora.enabled=true`, `quantization.enabled=false` | ~16GB  | Better quality          |
| Full      | Both disabled                                     | ~24GB+ | Best quality, multi-GPU |

### GPU Memory Usage (RTX 4070 Ti Super 16GB)

| Workload                         | VRAM        |
| -------------------------------- | ----------- |
| QLoRA Training                   | ~9GB        |
| Serving (unquantized)            | ~8GB        |
| Serving (quantized)              | ~4GB        |
| **Training + Quantized Serving** | **~13GB** âœ“ |

This allows demonstrating training and serving simultaneously on a single GPU.

### What Gets Logged to MLflow

**Parameters:**

- `model_name`, `training_method`, `batch_size`, `learning_rate`, `max_steps`
- `lora_r`, `lora_alpha`, `quantization_enabled`, `flash_attention`
- `dvc_data_version`

**Tags:**

- `argo_workflow_uid`, `docker_image_tag`, `prompt_name`, `prompt_version`

**Artifacts:**

- Model weights (LoRA adapters or full model)
- Processor configuration
- `prompt_info.json` (ensures serving uses training prompt)

______________________________________________________________________

<h2 id="serving">ğŸš€ Serving</h2>

### Start Serving

```bash
# Standard serving
serve run src.serving.serve:app_builder \
  model_uri="models:/dev.roco-radiology-vqa/1"

# With quantization (saves ~50% VRAM)
SERVE_QUANTIZED=true serve run src.serving.serve:app_builder \
  model_uri="models:/dev.roco-radiology-vqa/1"
```

### API Endpoints

| Endpoint   | Method | Description                  |
| ---------- | ------ | ---------------------------- |
| `/`        | GET    | Service info and links       |
| `/health`  | GET    | Kubernetes readiness probe   |
| `/info`    | GET    | Model metadata from MLflow   |
| `/predict` | POST   | Image analysis (file upload) |
| `/docs`    | GET    | Swagger UI                   |

### Usage Example

```bash
# Single image
curl -X POST http://localhost:8000/predict \
  -F "files=@chest_xray.jpg"

# Multiple images
curl -X POST http://localhost:8000/predict \
  -F "files=@image1.jpg" \
  -F "files=@image2.jpg"
```

**Response:**

```json
{
  "predictions": [
    {
      "image_index": 0,
      "filename": "chest_xray.jpg",
      "text": "Chest X-ray showing bilateral pulmonary infiltrates..."
    }
  ],
  "num_images": 1,
  "model_uri": "models:/dev.roco-radiology-vqa/1",
  "processing_time_ms": 1234.5
}
```

### Hot Model Reloading

Update the model without restarting the service:

```python
import requests

requests.post(
    "http://localhost:8000/reconfigure",
    json={"model_uri": "models:/dev.roco-radiology-vqa/2"},
)
```

______________________________________________________________________

<h2 id="project-structure">ğŸ“ Project Structure</h2>

```text
ai-radiology-qwen/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ training/                 # Training pipeline
â”‚   â”‚   â”œâ”€â”€ train.py              # Entry point (driver-worker pattern)
â”‚   â”‚   â”œâ”€â”€ config.py             # Configuration (env + YAML separation)
â”‚   â”‚   â”œâ”€â”€ trainer.py            # Custom HF Trainer with multi-LR
â”‚   â”‚   â”œâ”€â”€ callbacks.py          # Checkpoint callback (bundles prompt)
â”‚   â”‚   â”œâ”€â”€ data.py               # Dataset and collator (3D RoPE)
â”‚   â”‚   â”œâ”€â”€ dvc_loader.py         # Data versioning integration
â”‚   â”‚   â”œâ”€â”€ mlflow_wrapper.py     # PyFunc wrapper (key innovation)
â”‚   â”‚   â””â”€â”€ log.py                # JSON logging for observability
â”‚   â”‚
â”‚   â””â”€â”€ serving/                  # Inference API
â”‚       â”œâ”€â”€ serve.py              # Ray Serve + FastAPI
â”‚       â””â”€â”€ schemas.py            # Pydantic request/response models
â”‚
â”œâ”€â”€ configs/                      # Training configurations
â”‚   â”œâ”€â”€ debug_qlora.yaml          # Quick test (10 steps)
â”‚   â”œâ”€â”€ debug_qlora_flash.yaml    # With flash attention
â”‚   â”œâ”€â”€ debug_lora.yaml           # LoRA without quantization
â”‚   â””â”€â”€ demo.yaml                 # Demo configuration
â”‚
â”œâ”€â”€ data/                         # Sample data for testing
â”‚   â””â”€â”€ radiology_mini/
â”‚
â”œâ”€â”€ notebooks/                    # Exploration notebooks
â”‚
â”œâ”€â”€ .github/workflows/            # CI/CD
â”‚   â”œâ”€â”€ ci-code-quality.yaml      # Linting, formatting
â”‚   â”œâ”€â”€ ci-docker-build-push.yaml # Build container
â”‚   â””â”€â”€ train.yaml                # Trigger training
â”‚
â”œâ”€â”€ Dockerfile                    # Multi-stage build
â”œâ”€â”€ pyproject.toml                # Dependencies (uv)
â””â”€â”€ README.md                     # You are here
```

### Module Descriptions

| Module              | Purpose                                                             |
| ------------------- | ------------------------------------------------------------------- |
| `train.py`          | Orchestrates training: loads config, sets up MLflow, runs Ray Train |
| `config.py`         | Separates infrastructure (env) from training (YAML) configuration   |
| `trainer.py`        | Custom HuggingFace Trainer with per-component learning rates        |
| `callbacks.py`      | Bundles processor and prompt into checkpoints for serving           |
| `data.py`           | Handles Qwen's conversation format and 3D position encoding         |
| `mlflow_wrapper.py` | Enables MLflow to serve VLMs (the key innovation)                   |
| `serve.py`          | FastAPI application with health checks and batch inference          |

______________________________________________________________________

<h2 id="contributing">ğŸ‘¥ Contributing</h2>

Contributions are welcome! This project follows OpenCloudHub's contribution standards.

Please see our [Contributing Guidelines](https://github.com/opencloudhub/.github/blob/main/.github/CONTRIBUTING.md) and [Code of Conduct](https://github.com/opencloudhub/.github/blob/main/.github/CODE_OF_CONDUCT.md) for more details.

______________________________________________________________________

<h2 id="license">ğŸ“„ License</h2>

Distributed under the Apache 2.0 License. See [LICENSE](LICENSE) for more information.

______________________________________________________________________

<h2 id="contact">ğŸ“¬ Contact</h2>

Organization Link: [https://github.com/OpenCloudHub](https://github.com/OpenCloudHub)

Project Link: [https://github.com/opencloudhub/ai-radiology-qwen](https://github.com/opencloudhub/ai-radiology-qwen)

______________________________________________________________________

<h2 id="acknowledgements">ğŸ™ Acknowledgements</h2>

- [Official QwenVL Repo](https://github.com/QwenLM/Qwen3-VL/tree/main/qwen-vl-finetune)
- [Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct) - Vision-Language Model
- [MLflow](https://mlflow.org/) - ML lifecycle management
- [Ray](https://ray.io/) - Distributed training and serving
- [DVC](https://dvc.org/) - Data version control
- [PEFT](https://github.com/huggingface/peft) - Parameter-efficient fine-tuning
- [UV](https://github.com/astral-sh/uv) - Fast Python package manager

<p align="right">(<a href="#readme-top">back to top</a>)</p>
